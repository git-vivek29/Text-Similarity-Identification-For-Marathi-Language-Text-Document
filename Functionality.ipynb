{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the cells..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from indicnlp import common\n",
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "\n",
    "\n",
    "\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n",
    "\n",
    "# Add library to Python path\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "# Set environment variable for resources folder\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "\n",
    "\n",
    "def text_similarity(text1,text2):\n",
    "    #find the no. of docs in file1\n",
    "    file_docs = []\n",
    "    \n",
    "\n",
    "    \n",
    "    tokens = sentence_tokenize.sentence_split(text1, lang='mr')\n",
    "    #tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "    #print(\"Number of documents:\",len(file_docs))\n",
    "\n",
    "    #Tokenize each document\n",
    "    gen_docs = [[w.lower() for w in indic_tokenize.trivial_tokenize(text)] \n",
    "                for text in file_docs]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    #for doc in tf_idf[corpus]:\n",
    "        #print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "    \n",
    "    \n",
    "    # building the index\n",
    "    sims = gensim.similarities.Similarity('C:/Users/anike/NLP_mini_project/',tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "    #file2\n",
    "    file2_docs = []\n",
    "\n",
    "   \n",
    "    tokens = sentence_tokenize.sentence_split(text2, lang='mr')\n",
    "    #tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "            file2_docs.append(line)\n",
    "\n",
    "    #print(\"Number of documents:\",len(file2_docs))  \n",
    "    for line in file2_docs:\n",
    "        query_doc = [w.lower() for w in indic_tokenize.trivial_tokenize(line)]\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    \n",
    "\n",
    "    #Similarity\n",
    "\n",
    "\n",
    "    avg_sims = [] # array of averages\n",
    "\n",
    "    # for line in query documents\n",
    "    for line in file2_docs:\n",
    "            # tokenize words\n",
    "            query_doc = [w.lower() for w in indic_tokenize.trivial_tokenize(line)]\n",
    "            # create bag of words\n",
    "            query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "            # find similarity for each document\n",
    "            query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        \n",
    "            # calculate sum of similarities for each query doc\n",
    "            sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "            # calculate average of similarity for each query doc\n",
    "            avg = sum_of_sims / len(file_docs)\n",
    "    \n",
    "            # add average values into array\n",
    "            avg_sims.append(avg)  \n",
    "       # calculate total average\n",
    "    total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "        # round the value and multiply by 100 to format it as percentage\n",
    "    percentage_of_similarity = round(float(total_avg) * 100)\n",
    "        # if percentage is greater than 100\n",
    "        # that means documents are almost same\n",
    "    if percentage_of_similarity >= 100:\n",
    "        percentage_of_similarity = 100\n",
    "\n",
    "    return percentage_of_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
